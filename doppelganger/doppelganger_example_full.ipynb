{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doppelganger!\n",
    "Welcome to the Doppelganger example. If you have not already done so, please see the [README document](https://github.com/sidewalklabs/doppelganger/blob/master/README.md) for installation instructions and information on what Doppelganger is doing under the hood. For a simplified walkthrough, take a look at [doppelganger_example_simple](./doppelganger_example_simple.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in this Example?\n",
    "This workbook acts on a single PUMA and carries out the following tasks: \n",
    "1. Builds household- and person-specific Bayesian Networks for an individual PUMA; \n",
    "2. Allocates PUMS households to an individual PUMA consistent with subjectively-weighted marginal controls; and,\n",
    "3. Replaces the drawn PUMS population with synthetic people created by moving through and person-specific Bayesian Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping\n",
    "Before we get going, let's take care of some housekeeping tasks to make your Doppelganger Example experience as seamless as possible. \n",
    "\n",
    "We have included a cross-walk between PUMAs and Census tracts that we'll use later. Please navigate to this file and unzip it in the same directory. The file is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ./examples/sample_data/2010_puma_tract_mapping.txt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example operates on a single PUMA in California (`00106`). In Step 01 below, you'll see that we have already extracted PUMS data for this PUMA, so we'd recommend that you do not change to your favorite PUMA just yet (here's a set of [reference maps for California](https://www.census.gov/geo/maps-data/maps/2010puma/st06_ca.html) -- all states are available). But once you get comfortable with the tools, download the PUMS data for the PUMA of your choice, and go for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = '06'\n",
    "PUMA = '00106'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in the example we grab some Census data using the [Census API](https://www.census.gov/developers/). Enter your Census key below (if you need one, you can get one for free [here](http://api.census.gov/data/key_signup.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_CENSUS_KEY = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will generate a variety of output. Please specify where you'd like this data written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "We'll need to configure your computing environment and load in the configuration file before getting started. \n",
    "### Import the relevant Doppelganger Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from doppelganger import (\n",
    "    allocation,\n",
    "    inputs,\n",
    "    Configuration,\n",
    "    HouseholdAllocator,\n",
    "    PumsData,\n",
    "    SegmentedData,\n",
    "    BayesianNetworkModel,\n",
    "    Population,\n",
    "    Preprocessor,\n",
    "    Marginals\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Doppelganger example configuration file\n",
    "This file does the following three things:\n",
    "1. Defines person-specific variables in `person_fields`. In the example, you'll see `age`, `sex`, and `individual_income`. These variables are mapped to the PUMS variables in `inputs.py`. For example, `age` in Doppelganger is mapped to the PUMS variable `agep`. To use other variables from the PUMS with Doppelganger, you'll need to map their relationships in `inputs.py` and specify them here. \n",
    "2. Defines household-specific variables in `household_fields`. In the example, you'll see `household_income` and `num_vehicles`. As with the person-specific variables, you'll need to modify `inputs.py` to use other variables in Doppelganger.\n",
    "3. Defines procedures to process input variables into bins in `preprocessing`.\n",
    "4. Defines the structure of the household and person Bayesian Networks in `network_config_files`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = Configuration.from_file('sample_data/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `configuration` in hand, we can create a `preprocessor` object that will create methods to apply to the household and person PUMS data. In the current configuration, the `preprocessor` bins the individual income variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor.from_config(configuration.preprocessing_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01: Let's Build some Bayesian Networks!\n",
    "Bayesian Networks are built using the specification (`network_config_files`) in the configuration file and cleaned PUMS data. Up first: let's read in and clean the PUMS data.\n",
    "\n",
    "### Data reads\n",
    "#### Household Data\n",
    "The fields we need from the household data is the union of those defined in the `allocation.DEFAULT_HOUSEHOLD_FIELDS` and those defined `household_categories` section of the configuration file. The raw/dirty data collected for our example PUMA is available [here](https://www.census.gov/programs-surveys/acs/data/pums.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_fields = tuple(set(\n",
    "    field.name for field in allocation.DEFAULT_HOUSEHOLD_FIELDS).union(\n",
    "        set(configuration.household_fields)\n",
    "))\n",
    "\n",
    "households_data = PumsData.from_csv('sample_data/households_00106_dirty.csv').clean(\n",
    "    household_fields, preprocessor, puma=PUMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person Data\n",
    "The `allocation.DEFAULT_PERSON_FIELDS` defines a set of fields that can be used to create the `persons_data`; we take the union of these defaults with those defined in the `person_categories` section of the configuration file. This data is then extracted from the raw/dirty PUMS data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_fields = tuple(set(\n",
    "    field.name for field in allocation.DEFAULT_PERSON_FIELDS).union(\n",
    "        set(configuration.person_fields)\n",
    "))\n",
    "persons_data = PumsData.from_csv('sample_data/persons_00106_dirty.csv').clean(\n",
    "    persons_fields, preprocessor, puma=PUMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks\n",
    "The example below shows the training of a person-level model. It requires the following inputs:\n",
    "1. The `persons_data`, which was created in the previous step;\n",
    "2. A list of variables you want to consider in the training -- taken here from our configuration file; and,\n",
    "3. An optional weight if you want to weight the records (in the example, we use the PUMS variable `pwgtp`, which is defined in `inputs.py`. \n",
    "\n",
    "Once the training data is prepared, the training needs the `person_structure`, which is defined as in the `network_config_files` section of the configuration file, and the `person_fields`, which are also defined in the configuration file. The example `person_structure` specifies a network with three nodes (`age`, `sex`, and `income`) and two edges (`age` --> `income`, and `sex` --> `income`). The example `household_structure` specifies a network with three nodes (`num_people`, `household_income`, and `num_vehicles`) and three edges (`num_people` --> `household_income`, `num_people` --> `num_vehicles`, and `household_income` --> `num_vehicles`).\n",
    "\n",
    "<img src=\"./images/structure.png\" alt=\"Structure Image\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator.DESKTOP-D3SVMI5\\Desktop\\doppelganger\\doppelganger\\bayesnets.py:196: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  bayesian_network = BayesianNetwork.from_structure(data, structure)\n"
     ]
    }
   ],
   "source": [
    "person_training_data = SegmentedData.from_data(\n",
    "    persons_data,\n",
    "    list(configuration.person_fields),\n",
    "    weight_field = inputs.PERSON_WEIGHT.name\n",
    ")\n",
    "person_model = BayesianNetworkModel.train(\n",
    "    person_training_data,\n",
    "    configuration.person_structure,\n",
    "    configuration.person_fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian Networks can also be fully segmented. In the below example, we rebuild the networks, fully segmented by age category via the `segmenter` functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_segmentation = lambda x: x[inputs.AGE.name]\n",
    "\n",
    "person_training_data = SegmentedData.from_data(\n",
    "    persons_data,\n",
    "    list(configuration.person_fields),\n",
    "    inputs.PERSON_WEIGHT.name,\n",
    "    person_segmentation\n",
    ")\n",
    "person_model = BayesianNetworkModel.train(\n",
    "    person_training_data,\n",
    "    configuration.person_structure,\n",
    "    configuration.person_fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian Network can be written to disk and read from disk as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator.DESKTOP-D3SVMI5\\Desktop\\doppelganger\\doppelganger\\bayesnets.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  type_to_network[type_] = BayesianNetwork.from_json(json.dumps(network_json))\n"
     ]
    }
   ],
   "source": [
    "person_model_filename = os.path.join(output_dir, 'person_model.json')\n",
    "person_model.write(person_model_filename)\n",
    "person_model_reloaded = BayesianNetworkModel.from_file(person_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same steps as above, you can also build a household network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_segmenter = lambda x: x[inputs.NUM_PEOPLE.name]\n",
    "\n",
    "household_training_data = SegmentedData.from_data(\n",
    "    households_data,\n",
    "    list(configuration.household_fields),\n",
    "    inputs.HOUSEHOLD_WEIGHT.name,\n",
    "    household_segmenter,\n",
    ")\n",
    "household_model = BayesianNetworkModel.train(\n",
    "    household_training_data,\n",
    "    configuration.household_structure,\n",
    "    configuration.household_fields\n",
    ")\n",
    "\n",
    "household_model_filename = os.path.join(output_dir, 'household_model.json')\n",
    "household_model.write(household_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 02: Allocate PUMS households to the PUMA\n",
    "The sample data comes with a set of marginals that can be used in the allocation step. This file is named `sample_data/marginals_00106.csv`. To demonstrate how marginals can be created, we create them here. Note that this step will take a few minutes. If you want to skip it, use the `controls = Marginals.from_csv('sample_data/marginals_00106.csv')` call in the subsequent code box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MY_CENSUS_KEY:\n",
    "    new_marginal_filename = os.path.join(output_dir, 'new_marginals.csv')\n",
    "\n",
    "    with open('sample_data/2010_puma_tract_mapping.txt') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        marginals = Marginals.from_census_data(\n",
    "            csv_reader, MY_CENSUS_KEY, state=STATE, pumas=[PUMA]\n",
    "        )\n",
    "        marginals.write(new_marginal_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls = Marginals.from_csv(new_marginal_filename) # use this one to use your new marginals\n",
    "controls = Marginals.from_csv('sample_data/marginals_00106.csv') # use this one if you want to skip previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above marginal controls, the methods in `allocation.py` allocate discrete PUMS households to the subject PUMA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator.DESKTOP-D3SVMI5\\Desktop\\doppelganger\\doppelganger\\allocation.py:163: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  w = households[inputs.HOUSEHOLD_WEIGHT.name].as_matrix().T\n",
      "C:\\Users\\Administrator.DESKTOP-D3SVMI5\\Desktop\\doppelganger\\doppelganger\\allocation.py:174: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  hh_table = households[hh_columns].as_matrix()\n",
      "C:\\Users\\Administrator.DESKTOP-D3SVMI5\\Desktop\\doppelganger\\doppelganger\\allocation.py:176: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  A = tract_controls.data[hh_columns].as_matrix()\n",
      "G:\\Anaconda\\envs\\dop2\\lib\\site-packages\\cvxpy\\expressions\\expression.py:593: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 1 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "allocator = HouseholdAllocator.from_cleaned_data(controls, households_data, persons_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 03: Replace the PUMS Persons with Synthetic Persons created from the Bayesian Network\n",
    "It may be convenient to replace the source population with a synthetic population -- to add heterogeniety to the synthetic population or to obscure the source data set. In the below example we generate a set of persons and households using the `allocator` (the PUMS persons allocated to tracts), the Bayesian Networks (`person_model`, `household_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = Population.generate(\n",
    "    allocator, person_model, household_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the people and households as Pandas DataFrames and work with them directly. Households and people are unique by `(tract, serial_number, repeat_index)`. `serial_number` is the PUMS serialno for the household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         household_id   tract  serial_number  repeat_index  age sex  \\\n",
      "0    430101-1000481-0  430101        1000481             0  65+   M   \n",
      "1    430101-1000481-1  430101        1000481             1  65+   M   \n",
      "2    430101-1000481-2  430101        1000481             2  65+   M   \n",
      "3    430101-1000481-3  430101        1000481             3  65+   M   \n",
      "294  430101-1002740-0  430101        1002740             0  65+   F   \n",
      "\n",
      "    individual_income  \n",
      "0                 <=0  \n",
      "1                 <=0  \n",
      "2         40000-80000  \n",
      "3                 <=0  \n",
      "294               <=0  \n",
      "          household_id   tract  serial_number  repeat_index num_people  \\\n",
      "2472  430101-1000481-0  430101        1000481             0          1   \n",
      "2473  430101-1000481-1  430101        1000481             1          1   \n",
      "2474  430101-1000481-2  430101        1000481             2          1   \n",
      "2475  430101-1000481-3  430101        1000481             3          1   \n",
      "2476  430101-1002740-0  430101        1002740             0         4+   \n",
      "\n",
      "     household_income num_vehicles  \n",
      "2472          <=40000            0  \n",
      "2473          <=40000            1  \n",
      "2474          <=40000            1  \n",
      "2475          <=40000            1  \n",
      "2476           40000+            2  \n"
     ]
    }
   ],
   "source": [
    "people = population.generated_people\n",
    "households = population.generated_households\n",
    "\n",
    "sort_cols = [inputs.HOUSEHOLD_ID.name]\n",
    "\n",
    "print(people.sort_values(sort_cols).head())\n",
    "print(households.sort_values(sort_cols).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create one fat table of people and household attributes we can join on inputs.HOUSEHOLD_ID.name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       household_id tract_x  serial_number_x  repeat_index_x  age sex  \\\n",
      "0  430101-1000481-0  430101          1000481               0  65+   M   \n",
      "1  430101-1000481-1  430101          1000481               1  65+   M   \n",
      "2  430101-1000481-2  430101          1000481               2  65+   M   \n",
      "3  430101-1000481-3  430101          1000481               3  65+   M   \n",
      "4  430102-1000481-0  430102          1000481               0  65+   M   \n",
      "\n",
      "  individual_income tract_y  serial_number_y  repeat_index_y num_people  \\\n",
      "0               <=0  430101          1000481               0          1   \n",
      "1               <=0  430101          1000481               1          1   \n",
      "2       40000-80000  430101          1000481               2          1   \n",
      "3               <=0  430101          1000481               3          1   \n",
      "4               <=0  430102          1000481               0          1   \n",
      "\n",
      "  household_income num_vehicles  \n",
      "0          <=40000            0  \n",
      "1          <=40000            1  \n",
      "2          <=40000            1  \n",
      "3          <=40000            1  \n",
      "4          <=40000            1  \n"
     ]
    }
   ],
   "source": [
    "combined = pd.merge(people, households, on=[inputs.HOUSEHOLD_ID.name])\n",
    "\n",
    "print(combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or write them to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_people_filename = os.path.join(output_dir, 'generated_people.csv')\n",
    "generated_households_filename = os.path.join(output_dir, 'generated_households.csv')\n",
    "\n",
    "population.write(generated_people_filename, generated_households_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dop2",
   "language": "python",
   "name": "dop2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
